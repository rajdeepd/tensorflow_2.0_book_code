{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_REINFORCE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMhm17DcdG6pJAWaJmAfBAu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajdeepd/tensorflow_2.0_book_code/blob/master/ch09/Test_REINFORCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7R8WM6iGQCs",
        "outputId": "3e7149fd-a5cd-4ef1-92ce-b04bda6a1c3a"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'imageio==2.4.0'\n",
        "#!pip install pyvirtualdisplay\n",
        "!pip install tf-agents"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 784 kB in 3s (227 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 160983 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting imageio==2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/64/8e2bb6aac43d6ed7c2d9514320b43d5e80c00f150ee2b9408aee24359e6d/imageio-2.4.0.tar.gz (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (7.1.2)\n",
            "Building wheels for collected packages: imageio\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.0-cp37-none-any.whl size=3303881 sha256=ab89ab59c937bf5c5893c212a260ae3c053e103f029b6ebd94ed04edb90eff27\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/83/88/a1cba54ac06395d9e4ddcd9cf06911cd0b26cd78af9a61071b\n",
            "Successfully built imageio\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: imageio\n",
            "  Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "Successfully installed imageio-2.4.0\n",
            "Collecting tf-agents\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/cd/a0710b1caae042b7a4d54fc74073fb4df7adf073934798443bdc0059813a/tf_agents-0.7.1-py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (7.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.7.4.3)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.12.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.4.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.3->tf-agents) (54.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.16.0)\n",
            "Installing collected packages: tf-agents\n",
            "Successfully installed tf-agents-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k2WE0MzGVkk"
      },
      "source": [
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl.testing import parameterized\n",
        "from absl.testing.absltest import mock\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tf_agents.agents.reinforce import reinforce_agent\n",
        "from tf_agents.networks import actor_distribution_rnn_network\n",
        "from tf_agents.networks import network\n",
        "from tf_agents.networks import utils as network_utils\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import policy_step\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.utils import nest_utils"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB-AiwGgG4Hz"
      },
      "source": [
        "_obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n",
        "_time_step_spec = ts.time_step_spec(_obs_spec)\n",
        "_action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhRcxmL-HQCo"
      },
      "source": [
        "class DummyActorNet(network.Network):\n",
        "\n",
        "  def __init__(self,\n",
        "               input_tensor_spec,\n",
        "               output_tensor_spec,\n",
        "               unbounded_actions=False,\n",
        "               stateful=False):\n",
        "    # When unbounded_actions=True, we skip the final tanh activation and the\n",
        "    # action shift and scale. This allows us to compute the actor and critic\n",
        "    # losses by hand more easily.\n",
        "    # If stateful=True, the network state has the same shape as\n",
        "    # `input_tensor_spec`. Otherwise it is empty.\n",
        "    state_spec = (tf.TensorSpec(input_tensor_spec.shape, tf.float32)\n",
        "                  if stateful else ())\n",
        "    super(DummyActorNet, self).__init__(\n",
        "        input_tensor_spec=input_tensor_spec,\n",
        "        state_spec=state_spec,\n",
        "        name='DummyActorNet')\n",
        "    single_action_spec = tf.nest.flatten(output_tensor_spec)[0]\n",
        "    activation_fn = None if unbounded_actions else tf.nn.tanh\n",
        "    self._output_tensor_spec = output_tensor_spec\n",
        "    self._dummy_layers = [\n",
        "        tf.keras.layers.Dense(\n",
        "            single_action_spec.shape.num_elements() * 2,\n",
        "            activation=activation_fn,\n",
        "            kernel_initializer=tf.constant_initializer([[2, 1], [1, 1]]),\n",
        "            bias_initializer=tf.constant_initializer(5),\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "  def call(self, observations, step_type, network_state):\n",
        "    del step_type\n",
        "\n",
        "    states = tf.cast(tf.nest.flatten(observations)[0], tf.float32)\n",
        "    for layer in self._dummy_layers:\n",
        "      states = layer(states)\n",
        "\n",
        "    single_action_spec = tf.nest.flatten(self._output_tensor_spec)[0]\n",
        "    # action_spec is TensorSpec([1], ...) so make sure there's an outer dim.\n",
        "    actions = states[..., 0]\n",
        "    stdevs = states[..., 1]\n",
        "    actions = tf.reshape(actions, [-1] + single_action_spec.shape.as_list())\n",
        "    stdevs = tf.reshape(stdevs, [-1] + single_action_spec.shape.as_list())\n",
        "    actions = tf.nest.pack_sequence_as(self._output_tensor_spec, [actions])\n",
        "    stdevs = tf.nest.pack_sequence_as(self._output_tensor_spec, [stdevs])\n",
        "\n",
        "    distribution = nest_utils.map_structure_up_to(\n",
        "        self._output_tensor_spec,\n",
        "        tfp.distributions.MultivariateNormalDiag,\n",
        "        actions,\n",
        "        stdevs)\n",
        "    return distribution, network_state"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJPcNdwRGx51"
      },
      "source": [
        "agent = reinforce_agent.ReinforceAgent(\n",
        "        _time_step_spec,\n",
        "        _action_spec,\n",
        "        actor_network=DummyActorNet(\n",
        "            _obs_spec, _action_spec, unbounded_actions=False),\n",
        "        optimizer=None,\n",
        "    )\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY3Kte_XHf0i"
      },
      "source": [
        "observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
        "time_steps = ts.restart(observations, batch_size=2)\n",
        "actions = tf.constant([[0], [1]], dtype=tf.float32)\n",
        "actions_distribution = agent.collect_policy.distribution(\n",
        "        time_steps).action\n",
        "returns = tf.constant([1.9, 1.0], dtype=tf.float32)\n",
        "\n",
        "loss = agent.policy_gradient_loss(\n",
        "  actions_distribution, actions, time_steps.is_last(), returns, 1) "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiT_9UFABfoW",
        "outputId": "afeab182-b33d-47a3-c6f6-d4ae65ccec61"
      },
      "source": [
        "import sys\n",
        "\n",
        "tf.print(\"loss:\", loss, output_stream=sys.stdout)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 3.61492157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ori06gX0C3AG"
      },
      "source": [
        " Test that `policy_gradient_loss` reacts correctly to rewards when there are:\n",
        " * A single MDP episode\n",
        " * Returns on the `tf.StepType.FIRST` transitions\n",
        "\n",
        "F, L, M = `ts.StepType.{FIRST, MID, LAST}` in the chart below.\n",
        "\n",
        "```\n",
        "Experience looks like this:\n",
        "Trajectories: (F, L) -> (L, F)\n",
        "observation : [1, 2]    [1, 2]\n",
        "action      :   [0]       [1]\n",
        "reward      :    3         0\n",
        "~is_boundary:    1         0\n",
        "is_last     :    1         0\n",
        "valid reward:   3*1       4*0\n",
        "```\n",
        "\n",
        "The second action & reward should be masked out due to being on a boundary (step_type=(L, F)) transition.\n",
        "\n",
        "The expected_loss is > 0.0 in this case, only LAST should be excluded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU2C-HkeDMRO",
        "outputId": "1ce25551-a0c6-4598-abda-1b28e65b59de"
      },
      "source": [
        "step_type = tf.constant([ts.StepType.FIRST, ts.StepType.LAST])\n",
        "reward = tf.constant([3, 4], dtype=tf.float32)\n",
        "discount = tf.constant([1, 0], dtype=tf.float32)\n",
        "observations = tf.constant([[1, 2], [1, 2]], dtype=tf.float32)\n",
        "time_steps = ts.TimeStep(step_type, reward, discount, observations)\n",
        "\n",
        "actions = tf.constant([[0], [1]], dtype=tf.float32)\n",
        "actions_distribution = agent.collect_policy.distribution(\n",
        "        time_steps).action\n",
        "returns = tf.constant([3.0, 0.0], dtype=tf.float32)\n",
        "\n",
        "# Returns on the StepType.FIRST should be counted.\n",
        "expected_loss = 10.8935775757\n",
        "loss = agent.policy_gradient_loss(\n",
        "        actions_distribution, actions, time_steps.is_last(), returns, 1)\n",
        "tf.print(\"loss:\", loss, output_stream=sys.stdout)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 4.25681543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0KczfFcIn31"
      },
      "source": [
        "Bandit Episodes\n",
        "\n",
        "Sample which shows how train reacts correctly to experience when there is only a single Bandit episode.  Bandit episodes are encoded differently than MDP episodes.  They have only a single transition with `step_type=StepType.FIRST` and `next_step_type=StepType.LAST`.\n",
        "\n",
        "```\n",
        "F, L, M = ts.StepType.{FIRST, MID, LAST} in the chart below.\n",
        "\n",
        "Experience looks like this:\n",
        "Trajectories: (F, L)\n",
        "observation : [1, 2]\n",
        "action      :   [0]\n",
        "reward      :    3\n",
        "~is_boundary:    0\n",
        "is_last     :    1\n",
        "valid reward:   3*1\n",
        "```\n",
        "The single bandit transition is valid and not masked.\n",
        "\n",
        "The expected_loss is `> 0.0` in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBJtxZGKImiG",
        "outputId": "b18dd9fa-52ff-44af-9a59-a08f702b9586"
      },
      "source": [
        "step_type = tf.constant([ts.StepType.FIRST])\n",
        "next_step_type = tf.constant([ts.StepType.LAST])\n",
        "reward = tf.constant([3], dtype=tf.float32)\n",
        "discount = tf.constant([0], dtype=tf.float32)\n",
        "observations = tf.constant([[1, 2]], dtype=tf.float32)\n",
        "actions = tf.constant([[0]], dtype=tf.float32)\n",
        "\n",
        "experience = nest_utils.batch_nested_tensors(trajectory.Trajectory(\n",
        "        step_type, observations, actions, (), next_step_type, reward, discount))\n",
        "\n",
        "# Rewards should be counted.\n",
        "expected_loss = 10.8935775757\n",
        "\n",
        "#if tf.executing_eagerly():\n",
        "#      loss = lambda: agent.train(experience)\n",
        "#else:\n",
        "loss = lambda: agent.train(experience)\n",
        "tf.print(\"loss:\", loss, output_stream=sys.stdout)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: <function <lambda> at 0x7f96207265f0>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}