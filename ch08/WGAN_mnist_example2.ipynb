{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN_mnist_example2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9bBxoLT09k18gZQuYO7zX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajdeepd/tensorflow_2.0_book_code/blob/master/ch08/WGAN_mnist_example2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBWSmkvm4f9K"
      },
      "source": [
        "# example of a wgan for generating handwritten digits\n",
        "from numpy import expand_dims\n",
        "from numpy import mean\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.mnist import load_data\n",
        "from keras import backend\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.constraints import Constraint\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzRLjn8d45xV"
      },
      "source": [
        "# clip model weights to a given hypercube\n",
        "class ClipConstraint(Constraint):\n",
        "\t# set clip value when initialized\n",
        "\tdef __init__(self, clip_value):\n",
        "\t\tself.clip_value = clip_value\n",
        "\n",
        "\t# clip model weights to hypercube\n",
        "\tdef __call__(self, weights):\n",
        "\t\treturn backend.clip(weights, -self.clip_value, self.clip_value)\n",
        "\n",
        "\t# get the config\n",
        "\tdef get_config(self):\n",
        "\t\treturn {'clip_value': self.clip_value}\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8De_MLNz4_X7"
      },
      "source": [
        "# calculate wasserstein loss\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "\treturn backend.mean(y_true * y_pred)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7_QS_Ff5Fpj"
      },
      "source": [
        "# define the standalone critic model\n",
        "def define_critic(in_shape=(28,28,1)):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# weight constraint\n",
        "\tconst = ClipConstraint(0.01)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\t# downsample to 14x14\n",
        "\tmodel.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const, input_shape=in_shape))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# downsample to 7x7\n",
        "\tmodel.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, kernel_constraint=const))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# scoring, linear activation\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(1))\n",
        "\t# compile model\n",
        "\topt = RMSprop(lr=0.00005)\n",
        "\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "# define the standalone generator model\n",
        "def define_generator(latent_dim):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\t# foundation for 7x7 image\n",
        "\tn_nodes = 128 * 7 * 7\n",
        "\tmodel.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Reshape((7, 7, 128)))\n",
        "\t# upsample to 14x14\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# upsample to 28x28\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# output 28x28x1\n",
        "\tmodel.add(Conv2D(1, (7,7), activation='tanh', padding='same', kernel_initializer=init))\n",
        "\treturn model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2LmRAc_5Oe4"
      },
      "source": [
        "# define the combined generator and critic model, for updating the generator\n",
        "def define_gan(generator, critic):\n",
        "\t# make weights in the critic not trainable\n",
        "\tcritic.trainable = False\n",
        "\t# connect them\n",
        "\tmodel = Sequential()\n",
        "\t# add generator\n",
        "\tmodel.add(generator)\n",
        "\t# add the critic\n",
        "\tmodel.add(critic)\n",
        "\t# compile model\n",
        "\topt = RMSprop(lr=0.00005)\n",
        "\tmodel.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\treturn model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfNcKECN5UEW"
      },
      "source": [
        "# load images\n",
        "def load_real_samples():\n",
        "\t# load dataset\n",
        "\t(trainX, trainy), (_, _) = load_data()\n",
        "\t# select all of the examples for a given class\n",
        "\tselected_ix = trainy == 7\n",
        "\tX = trainX[selected_ix]\n",
        "\t# expand to 3d, e.g. add channels\n",
        "\tX = expand_dims(X, axis=-1)\n",
        "\t# convert from ints to floats\n",
        "\tX = X.astype('float32')\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX = (X - 127.5) / 127.5\n",
        "\treturn X"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EghSvzEo5hG-"
      },
      "source": [
        "# select real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# choose random instances\n",
        "\tix = randint(0, dataset.shape[0], n_samples)\n",
        "\t# select images\n",
        "\tX = dataset[ix]\n",
        "\t# generate class labels, -1 for 'real'\n",
        "\ty = -ones((n_samples, 1))\n",
        "\treturn X, y\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yw2Ez36KuqZ"
      },
      "source": [
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4Zii7dvKy2r"
      },
      "source": [
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "\t# generate points in latent space\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\tX = generator.predict(x_input)\n",
        "\t# create class labels with 1.0 for 'fake'\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn X, y"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xITwykPOK3O3"
      },
      "source": [
        "# generate samples and save as a plot and save the model\n",
        "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
        "\t# prepare fake examples\n",
        "\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "\t# scale from [-1,1] to [0,1]\n",
        "\tX = (X + 1) / 2.0\n",
        "\t# plot images\n",
        "\tfor i in range(10 * 10):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(10, 10, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
        "\t# save plot to file\n",
        "\tfilename1 = 'generated_plot_%04d.png' % (step+1)\n",
        "\tpyplot.savefig(filename1)\n",
        "\tpyplot.close()\n",
        "\t# save the generator model\n",
        "\tfilename2 = 'model_%04d.h5' % (step+1)\n",
        "\tg_model.save(filename2)\n",
        "\tprint('>Saved: %s and %s' % (filename1, filename2))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhmj3qmHK4vW"
      },
      "source": [
        "# create a line plot of loss for the gan and save to file\n",
        "%matplotlib inline\n",
        "def plot_history(d1_hist, d2_hist, g_hist):\n",
        "\t# plot history\n",
        "\tpyplot.plot(d1_hist, label='crit_real')\n",
        "\tpyplot.plot(d2_hist, label='crit_fake')\n",
        "\tpyplot.plot(g_hist, label='gen')\n",
        "\tpyplot.legend()\n",
        "\tpyplot.savefig('plot_line_plot_loss.png')\n",
        "\tpyplot.close()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYnFW4OOLCwN"
      },
      "source": [
        "# train the generator and critic\n",
        "def train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=10, n_batch=64, n_critic=5):\n",
        "\t# calculate the number of batches per training epoch\n",
        "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "\t# calculate the number of training iterations\n",
        "\tn_steps = bat_per_epo * n_epochs\n",
        "\t# calculate the size of half a batch of samples\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t# lists for keeping track of loss\n",
        "\tc1_hist, c2_hist, g_hist = list(), list(), list()\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_steps):\n",
        "\t\t# update the critic more than the generator\n",
        "\t\tc1_tmp, c2_tmp = list(), list()\n",
        "\t\tfor _ in range(n_critic):\n",
        "\t\t\t# get randomly selected 'real' samples\n",
        "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t\t# update critic model weights\n",
        "\t\t\tc_loss1 = c_model.train_on_batch(X_real, y_real)\n",
        "\t\t\tc1_tmp.append(c_loss1)\n",
        "\t\t\t# generate 'fake' examples\n",
        "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t\t# update critic model weights\n",
        "\t\t\tc_loss2 = c_model.train_on_batch(X_fake, y_fake)\n",
        "\t\t\tc2_tmp.append(c_loss2)\n",
        "\t\t# store critic loss\n",
        "\t\tc1_hist.append(mean(c1_tmp))\n",
        "\t\tc2_hist.append(mean(c2_tmp))\n",
        "\t\t# prepare points in latent space as input for the generator\n",
        "\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t# create inverted labels for the fake samples\n",
        "\t\ty_gan = -ones((n_batch, 1))\n",
        "\t\t# update the generator via the critic's error\n",
        "\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\t\tg_hist.append(g_loss)\n",
        "\t\t# summarize loss on this batch\n",
        "\t\tprint('>%d, c1=%.3f, c2=%.3f g=%.3f' % (i+1, c1_hist[-1], c2_hist[-1], g_loss))\n",
        "\t\t# evaluate the model performance every 'epoch'\n",
        "\t\tif (i+1) % bat_per_epo == 0:\n",
        "\t\t\tsummarize_performance(i, g_model, latent_dim)\n",
        "\t# line plots of loss\n",
        "\tplot_history(c1_hist, c2_hist, g_hist)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdy-lYlHo9eK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyZTceybLGxr",
        "outputId": "bbc41e7e-b06c-4ba3-fe4b-bef3ae688a5a"
      },
      "source": [
        "# size of the latent space\n",
        "latent_dim = 50\n",
        "# create the critic\n",
        "critic = define_critic()\n",
        "# create the generator\n",
        "generator = define_generator(latent_dim)\n",
        "# create the gan\n",
        "gan_model = define_gan(generator, critic)\n",
        "# load image data\n",
        "dataset = load_real_samples()\n",
        "print(dataset.shape)\n",
        "# train model\n",
        "train(generator, critic, gan_model, dataset, latent_dim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6265, 28, 28, 1)\n",
            ">1, c1=-1.663, c2=-0.004 g=-0.002\n",
            ">2, c1=-5.596, c2=0.058 g=-0.011\n",
            ">3, c1=-8.529, c2=0.108 g=-0.019\n",
            ">4, c1=-10.916, c2=0.156 g=-0.029\n",
            ">5, c1=-13.454, c2=0.203 g=-0.038\n",
            ">6, c1=-15.663, c2=0.230 g=-0.045\n",
            ">7, c1=-17.030, c2=0.244 g=-0.053\n",
            ">8, c1=-18.464, c2=0.278 g=-0.066\n",
            ">9, c1=-20.215, c2=0.303 g=-0.075\n",
            ">10, c1=-20.867, c2=0.363 g=-0.090\n",
            ">11, c1=-21.710, c2=0.441 g=-0.108\n",
            ">12, c1=-22.533, c2=0.527 g=-0.132\n",
            ">13, c1=-24.077, c2=0.648 g=-0.158\n",
            ">14, c1=-24.854, c2=0.803 g=-0.190\n",
            ">15, c1=-25.165, c2=0.906 g=-0.230\n",
            ">16, c1=-26.454, c2=1.058 g=-0.266\n",
            ">17, c1=-26.721, c2=1.142 g=-0.310\n",
            ">18, c1=-27.757, c2=1.260 g=-0.362\n",
            ">19, c1=-28.316, c2=1.383 g=-0.418\n",
            ">20, c1=-28.381, c2=1.436 g=-0.477\n",
            ">21, c1=-29.340, c2=1.517 g=-0.546\n",
            ">22, c1=-29.883, c2=1.531 g=-0.624\n",
            ">23, c1=-30.159, c2=1.574 g=-0.680\n",
            ">24, c1=-30.692, c2=1.543 g=-0.785\n",
            ">25, c1=-31.447, c2=1.532 g=-0.881\n",
            ">26, c1=-31.955, c2=1.464 g=-0.990\n",
            ">27, c1=-31.939, c2=1.309 g=-1.139\n",
            ">28, c1=-33.212, c2=1.207 g=-1.221\n",
            ">29, c1=-33.169, c2=0.850 g=-1.401\n",
            ">30, c1=-33.392, c2=0.591 g=-1.536\n",
            ">31, c1=-33.385, c2=0.204 g=-1.726\n",
            ">32, c1=-34.327, c2=-0.134 g=-1.942\n",
            ">33, c1=-34.732, c2=-0.543 g=-2.185\n",
            ">34, c1=-35.005, c2=-1.183 g=-2.404\n",
            ">35, c1=-35.320, c2=-1.897 g=-2.684\n",
            ">36, c1=-35.840, c2=-2.738 g=-3.008\n",
            ">37, c1=-36.153, c2=-3.546 g=-3.382\n",
            ">38, c1=-36.852, c2=-4.387 g=-3.722\n",
            ">39, c1=-37.214, c2=-5.309 g=-4.388\n",
            ">40, c1=-36.883, c2=-5.897 g=-4.962\n",
            ">41, c1=-38.584, c2=-6.391 g=-5.555\n",
            ">42, c1=-38.224, c2=-7.325 g=-6.213\n",
            ">43, c1=-39.102, c2=-8.057 g=-7.086\n",
            ">44, c1=-38.322, c2=-8.472 g=-7.893\n",
            ">45, c1=-39.715, c2=-8.416 g=-9.177\n",
            ">46, c1=-40.240, c2=-8.785 g=-10.671\n",
            ">47, c1=-39.231, c2=-9.056 g=-12.201\n",
            ">48, c1=-40.436, c2=-9.481 g=-13.614\n",
            ">49, c1=-42.074, c2=-9.832 g=-15.030\n",
            ">50, c1=-41.632, c2=-10.773 g=-16.073\n",
            ">51, c1=-41.195, c2=-11.986 g=-17.912\n",
            ">52, c1=-42.002, c2=-12.566 g=-19.789\n",
            ">53, c1=-42.029, c2=-14.012 g=-21.655\n",
            ">54, c1=-42.981, c2=-14.864 g=-24.076\n",
            ">55, c1=-43.465, c2=-16.019 g=-26.576\n",
            ">56, c1=-44.598, c2=-16.148 g=-28.496\n",
            ">57, c1=-44.213, c2=-17.743 g=-30.532\n",
            ">58, c1=-45.264, c2=-19.129 g=-33.352\n",
            ">59, c1=-45.681, c2=-21.344 g=-36.751\n",
            ">60, c1=-45.476, c2=-21.849 g=-40.030\n",
            ">61, c1=-47.046, c2=-22.852 g=-42.980\n",
            ">62, c1=-46.415, c2=-24.954 g=-46.217\n",
            ">63, c1=-47.330, c2=-26.007 g=-48.348\n",
            ">64, c1=-48.167, c2=-27.723 g=-53.234\n",
            ">65, c1=-48.922, c2=-29.724 g=-57.674\n",
            ">66, c1=-49.235, c2=-30.306 g=-58.462\n",
            ">67, c1=-50.052, c2=-31.399 g=-63.017\n",
            ">68, c1=-50.772, c2=-32.727 g=-67.089\n",
            ">69, c1=-52.171, c2=-34.002 g=-69.242\n",
            ">70, c1=-52.482, c2=-35.555 g=-71.282\n",
            ">71, c1=-53.499, c2=-36.851 g=-75.194\n",
            ">72, c1=-53.743, c2=-37.431 g=-79.785\n",
            ">73, c1=-54.022, c2=-39.093 g=-83.863\n",
            ">74, c1=-55.567, c2=-40.136 g=-85.514\n",
            ">75, c1=-54.995, c2=-41.121 g=-83.419\n",
            ">76, c1=-56.119, c2=-42.491 g=-87.688\n",
            ">77, c1=-57.450, c2=-43.262 g=-89.941\n",
            ">78, c1=-58.146, c2=-44.164 g=-93.342\n",
            ">79, c1=-58.595, c2=-45.519 g=-90.710\n",
            ">80, c1=-58.459, c2=-46.412 g=-98.639\n",
            ">81, c1=-60.044, c2=-47.000 g=-95.618\n",
            ">82, c1=-60.512, c2=-47.970 g=-98.783\n",
            ">83, c1=-61.547, c2=-49.239 g=-104.045\n",
            ">84, c1=-61.362, c2=-50.144 g=-110.579\n",
            ">85, c1=-63.267, c2=-50.691 g=-101.236\n",
            ">86, c1=-63.970, c2=-51.932 g=-106.981\n",
            ">87, c1=-63.698, c2=-52.733 g=-108.129\n",
            ">88, c1=-64.405, c2=-53.276 g=-110.651\n",
            ">89, c1=-65.412, c2=-54.252 g=-112.401\n",
            ">90, c1=-66.383, c2=-55.270 g=-107.580\n",
            ">91, c1=-67.644, c2=-56.387 g=-109.225\n",
            ">92, c1=-67.827, c2=-57.013 g=-115.142\n",
            ">93, c1=-69.003, c2=-58.160 g=-114.059\n",
            ">94, c1=-68.996, c2=-58.694 g=-115.990\n",
            ">95, c1=-69.986, c2=-59.444 g=-119.218\n",
            ">96, c1=-71.346, c2=-60.182 g=-116.771\n",
            ">97, c1=-71.005, c2=-61.173 g=-114.247\n",
            ">Saved: generated_plot_0097.png and model_0097.h5\n",
            ">98, c1=-71.425, c2=-61.793 g=-113.270\n",
            ">99, c1=-73.085, c2=-62.649 g=-120.347\n",
            ">100, c1=-73.483, c2=-63.461 g=-116.769\n",
            ">101, c1=-74.198, c2=-64.228 g=-117.484\n",
            ">102, c1=-74.685, c2=-64.971 g=-120.803\n",
            ">103, c1=-75.662, c2=-65.657 g=-117.434\n",
            ">104, c1=-77.087, c2=-66.490 g=-127.766\n",
            ">105, c1=-76.914, c2=-66.905 g=-119.351\n",
            ">106, c1=-77.992, c2=-67.799 g=-127.447\n",
            ">107, c1=-78.531, c2=-68.763 g=-128.604\n",
            ">108, c1=-78.359, c2=-69.354 g=-130.497\n",
            ">109, c1=-79.961, c2=-70.082 g=-135.954\n",
            ">110, c1=-80.547, c2=-70.848 g=-129.277\n",
            ">111, c1=-80.676, c2=-71.572 g=-128.854\n",
            ">112, c1=-82.879, c2=-72.240 g=-132.122\n",
            ">113, c1=-82.672, c2=-73.046 g=-133.203\n",
            ">114, c1=-83.115, c2=-73.574 g=-131.019\n",
            ">115, c1=-84.068, c2=-74.447 g=-136.221\n",
            ">116, c1=-85.257, c2=-75.102 g=-140.757\n",
            ">117, c1=-85.458, c2=-75.735 g=-148.886\n",
            ">118, c1=-87.016, c2=-76.502 g=-141.662\n",
            ">119, c1=-87.024, c2=-76.999 g=-138.358\n",
            ">120, c1=-88.171, c2=-77.853 g=-135.577\n",
            ">121, c1=-88.526, c2=-78.556 g=-142.929\n",
            ">122, c1=-88.818, c2=-79.185 g=-156.282\n",
            ">123, c1=-90.176, c2=-79.976 g=-141.008\n",
            ">124, c1=-90.294, c2=-80.670 g=-142.456\n",
            ">125, c1=-90.779, c2=-81.306 g=-146.219\n",
            ">126, c1=-91.846, c2=-81.991 g=-141.570\n",
            ">127, c1=-92.351, c2=-82.600 g=-157.686\n",
            ">128, c1=-93.582, c2=-83.386 g=-146.773\n",
            ">129, c1=-94.992, c2=-84.054 g=-154.361\n",
            ">130, c1=-95.538, c2=-84.766 g=-158.491\n",
            ">131, c1=-95.778, c2=-85.454 g=-143.982\n",
            ">132, c1=-96.800, c2=-86.096 g=-157.427\n",
            ">133, c1=-97.153, c2=-86.828 g=-154.707\n",
            ">134, c1=-98.307, c2=-87.464 g=-157.237\n",
            ">135, c1=-98.585, c2=-88.179 g=-153.841\n",
            ">136, c1=-100.280, c2=-88.892 g=-164.919\n",
            ">137, c1=-100.174, c2=-89.582 g=-148.023\n",
            ">138, c1=-101.570, c2=-90.323 g=-148.190\n",
            ">139, c1=-102.119, c2=-90.985 g=-156.930\n",
            ">140, c1=-102.814, c2=-91.703 g=-172.005\n",
            ">141, c1=-102.901, c2=-92.330 g=-149.067\n",
            ">142, c1=-104.285, c2=-93.061 g=-163.447\n",
            ">143, c1=-104.627, c2=-93.722 g=-160.498\n",
            ">144, c1=-105.652, c2=-94.424 g=-161.359\n",
            ">145, c1=-106.137, c2=-95.150 g=-165.567\n",
            ">146, c1=-107.933, c2=-95.869 g=-166.679\n",
            ">147, c1=-108.966, c2=-96.600 g=-174.951\n",
            ">148, c1=-108.625, c2=-97.306 g=-169.879\n",
            ">149, c1=-109.907, c2=-98.071 g=-168.087\n",
            ">150, c1=-109.240, c2=-98.782 g=-176.471\n",
            ">151, c1=-111.217, c2=-99.523 g=-176.106\n",
            ">152, c1=-111.301, c2=-100.210 g=-166.021\n",
            ">153, c1=-112.884, c2=-100.956 g=-176.928\n",
            ">154, c1=-113.405, c2=-101.652 g=-174.127\n",
            ">155, c1=-113.760, c2=-102.404 g=-185.384\n",
            ">156, c1=-115.126, c2=-103.176 g=-191.018\n",
            ">157, c1=-115.770, c2=-103.889 g=-186.454\n",
            ">158, c1=-115.874, c2=-104.631 g=-187.759\n",
            ">159, c1=-116.637, c2=-105.344 g=-187.468\n",
            ">160, c1=-117.108, c2=-106.059 g=-183.143\n",
            ">161, c1=-118.634, c2=-106.813 g=-198.297\n",
            ">162, c1=-119.651, c2=-107.535 g=-187.826\n",
            ">163, c1=-119.913, c2=-108.263 g=-178.255\n",
            ">164, c1=-121.856, c2=-108.956 g=-184.302\n",
            ">165, c1=-121.303, c2=-109.707 g=-204.386\n",
            ">166, c1=-122.619, c2=-110.495 g=-203.470\n",
            ">167, c1=-123.740, c2=-111.241 g=-189.310\n",
            ">168, c1=-124.226, c2=-111.974 g=-194.780\n",
            ">169, c1=-125.254, c2=-112.744 g=-206.732\n",
            ">170, c1=-126.235, c2=-113.493 g=-190.913\n",
            ">171, c1=-127.128, c2=-114.220 g=-198.516\n",
            ">172, c1=-127.137, c2=-114.994 g=-202.710\n",
            ">173, c1=-128.585, c2=-115.785 g=-195.101\n",
            ">174, c1=-128.444, c2=-116.535 g=-198.564\n",
            ">175, c1=-130.074, c2=-117.298 g=-208.588\n",
            ">176, c1=-131.470, c2=-118.078 g=-208.243\n",
            ">177, c1=-131.003, c2=-118.808 g=-206.136\n",
            ">178, c1=-132.399, c2=-119.580 g=-208.277\n",
            ">179, c1=-132.641, c2=-120.321 g=-210.980\n",
            ">180, c1=-133.378, c2=-121.141 g=-202.420\n",
            ">181, c1=-133.858, c2=-121.836 g=-210.552\n",
            ">182, c1=-135.647, c2=-122.671 g=-209.957\n",
            ">183, c1=-135.956, c2=-123.416 g=-228.630\n",
            ">184, c1=-136.643, c2=-124.193 g=-205.085\n",
            ">185, c1=-137.366, c2=-124.962 g=-224.311\n",
            ">186, c1=-139.162, c2=-125.765 g=-241.307\n",
            ">187, c1=-139.065, c2=-126.519 g=-205.690\n",
            ">188, c1=-139.668, c2=-127.280 g=-238.505\n",
            ">189, c1=-140.739, c2=-128.060 g=-219.876\n",
            ">190, c1=-141.316, c2=-128.809 g=-219.387\n",
            ">191, c1=-142.940, c2=-129.633 g=-234.482\n",
            ">192, c1=-142.870, c2=-130.391 g=-232.649\n",
            ">193, c1=-144.141, c2=-131.217 g=-244.749\n",
            ">194, c1=-145.321, c2=-131.981 g=-246.030\n",
            ">Saved: generated_plot_0194.png and model_0194.h5\n",
            ">195, c1=-146.095, c2=-132.765 g=-258.849\n",
            ">196, c1=-146.795, c2=-133.541 g=-219.400\n",
            ">197, c1=-147.506, c2=-134.346 g=-255.424\n",
            ">198, c1=-148.166, c2=-135.137 g=-252.455\n",
            ">199, c1=-149.307, c2=-135.947 g=-245.236\n",
            ">200, c1=-149.575, c2=-136.736 g=-238.970\n",
            ">201, c1=-149.994, c2=-137.475 g=-251.049\n",
            ">202, c1=-152.012, c2=-138.316 g=-231.054\n",
            ">203, c1=-152.493, c2=-139.088 g=-248.319\n",
            ">204, c1=-153.079, c2=-139.912 g=-247.721\n",
            ">205, c1=-154.895, c2=-140.706 g=-243.741\n",
            ">206, c1=-155.543, c2=-141.515 g=-243.958\n",
            ">207, c1=-156.168, c2=-142.332 g=-249.970\n",
            ">208, c1=-156.259, c2=-143.097 g=-240.604\n",
            ">209, c1=-156.313, c2=-143.901 g=-263.698\n",
            ">210, c1=-158.250, c2=-144.721 g=-256.641\n",
            ">211, c1=-159.391, c2=-145.444 g=-259.627\n",
            ">212, c1=-159.551, c2=-146.303 g=-268.906\n",
            ">213, c1=-161.486, c2=-147.139 g=-259.662\n",
            ">214, c1=-162.034, c2=-147.961 g=-268.178\n",
            ">215, c1=-162.543, c2=-148.754 g=-282.345\n",
            ">216, c1=-162.689, c2=-149.572 g=-278.683\n",
            ">217, c1=-164.477, c2=-150.378 g=-270.508\n",
            ">218, c1=-164.411, c2=-151.184 g=-268.478\n",
            ">219, c1=-166.333, c2=-152.021 g=-275.375\n",
            ">220, c1=-166.600, c2=-152.807 g=-271.573\n",
            ">221, c1=-167.869, c2=-153.614 g=-287.863\n",
            ">222, c1=-167.110, c2=-154.423 g=-265.942\n",
            ">223, c1=-169.141, c2=-155.283 g=-267.950\n",
            ">224, c1=-170.138, c2=-156.066 g=-285.518\n",
            ">225, c1=-170.575, c2=-156.855 g=-289.517\n",
            ">226, c1=-172.018, c2=-157.746 g=-308.820\n",
            ">227, c1=-173.441, c2=-158.591 g=-295.930\n",
            ">228, c1=-173.525, c2=-159.418 g=-314.030\n",
            ">229, c1=-174.628, c2=-160.212 g=-300.998\n",
            ">230, c1=-175.529, c2=-161.024 g=-294.649\n",
            ">231, c1=-174.932, c2=-161.847 g=-281.929\n",
            ">232, c1=-176.800, c2=-162.697 g=-295.201\n",
            ">233, c1=-177.831, c2=-163.477 g=-293.096\n",
            ">234, c1=-178.987, c2=-164.329 g=-291.891\n",
            ">235, c1=-179.239, c2=-165.158 g=-285.837\n",
            ">236, c1=-181.090, c2=-166.014 g=-311.602\n",
            ">237, c1=-180.209, c2=-166.790 g=-326.950\n",
            ">238, c1=-181.192, c2=-167.678 g=-307.870\n",
            ">239, c1=-183.780, c2=-168.523 g=-304.745\n",
            ">240, c1=-183.643, c2=-169.380 g=-315.274\n",
            ">241, c1=-185.189, c2=-170.236 g=-327.300\n",
            ">242, c1=-186.199, c2=-171.039 g=-332.755\n",
            ">243, c1=-186.707, c2=-171.873 g=-285.006\n",
            ">244, c1=-186.961, c2=-172.736 g=-323.001\n",
            ">245, c1=-188.261, c2=-173.518 g=-303.625\n",
            ">246, c1=-189.320, c2=-174.375 g=-294.503\n",
            ">247, c1=-188.108, c2=-175.125 g=-314.279\n",
            ">248, c1=-191.096, c2=-176.027 g=-308.700\n",
            ">249, c1=-193.490, c2=-176.923 g=-317.996\n",
            ">250, c1=-193.035, c2=-177.743 g=-310.119\n",
            ">251, c1=-193.218, c2=-178.570 g=-314.710\n",
            ">252, c1=-193.879, c2=-179.429 g=-316.765\n",
            ">253, c1=-194.676, c2=-180.298 g=-313.714\n",
            ">254, c1=-197.499, c2=-181.172 g=-349.236\n",
            ">255, c1=-197.178, c2=-182.050 g=-324.057\n",
            ">256, c1=-198.855, c2=-182.867 g=-320.172\n",
            ">257, c1=-198.927, c2=-183.764 g=-329.820\n",
            ">258, c1=-199.748, c2=-184.603 g=-325.391\n",
            ">259, c1=-200.995, c2=-185.456 g=-361.747\n",
            ">260, c1=-201.309, c2=-186.319 g=-343.136\n",
            ">261, c1=-203.367, c2=-187.236 g=-383.568\n",
            ">262, c1=-203.145, c2=-188.050 g=-325.994\n",
            ">263, c1=-203.825, c2=-188.918 g=-331.303\n",
            ">264, c1=-205.422, c2=-189.773 g=-342.299\n",
            ">265, c1=-206.484, c2=-190.677 g=-328.827\n",
            ">266, c1=-206.916, c2=-191.549 g=-370.794\n",
            ">267, c1=-207.660, c2=-192.392 g=-357.247\n",
            ">268, c1=-209.325, c2=-193.336 g=-373.548\n",
            ">269, c1=-209.076, c2=-194.211 g=-341.492\n",
            ">270, c1=-210.804, c2=-195.052 g=-358.880\n",
            ">271, c1=-211.423, c2=-195.875 g=-340.568\n",
            ">272, c1=-212.645, c2=-196.838 g=-351.484\n",
            ">273, c1=-214.073, c2=-197.684 g=-353.616\n",
            ">274, c1=-213.157, c2=-198.538 g=-381.873\n",
            ">275, c1=-215.067, c2=-199.431 g=-351.016\n",
            ">276, c1=-216.166, c2=-200.424 g=-388.730\n",
            ">277, c1=-217.475, c2=-201.265 g=-379.284\n",
            ">278, c1=-217.962, c2=-201.920 g=-375.789\n",
            ">279, c1=-219.188, c2=-202.884 g=-346.583\n",
            ">280, c1=-219.975, c2=-203.555 g=-404.040\n",
            ">281, c1=-220.128, c2=-204.753 g=-368.760\n",
            ">282, c1=-220.717, c2=-205.629 g=-355.081\n",
            ">283, c1=-222.269, c2=-206.408 g=-418.128\n",
            ">284, c1=-222.553, c2=-206.826 g=-379.688\n",
            ">285, c1=-223.301, c2=-207.653 g=-384.346\n",
            ">286, c1=-224.481, c2=-208.510 g=-360.890\n",
            ">287, c1=-225.726, c2=-209.957 g=-378.396\n",
            ">288, c1=-227.191, c2=-210.669 g=-367.991\n",
            ">289, c1=-227.260, c2=-211.632 g=-362.230\n",
            ">290, c1=-228.136, c2=-212.791 g=-412.669\n",
            ">291, c1=-228.874, c2=-212.665 g=-384.299\n",
            ">Saved: generated_plot_0291.png and model_0291.h5\n",
            ">292, c1=-230.585, c2=-213.521 g=-349.234\n",
            ">293, c1=-231.471, c2=-215.257 g=-354.496\n",
            ">294, c1=-233.122, c2=-215.391 g=-374.905\n",
            ">295, c1=-233.368, c2=-216.456 g=-380.821\n",
            ">296, c1=-234.433, c2=-218.020 g=-399.903\n",
            ">297, c1=-234.069, c2=-216.957 g=-356.076\n",
            ">298, c1=-235.500, c2=-218.664 g=-343.036\n",
            ">299, c1=-234.551, c2=-219.241 g=-356.994\n",
            ">300, c1=-237.360, c2=-219.679 g=-359.727\n",
            ">301, c1=-237.812, c2=-220.152 g=-356.077\n",
            ">302, c1=-238.500, c2=-220.864 g=-341.986\n",
            ">303, c1=-239.413, c2=-221.554 g=-332.582\n",
            ">304, c1=-239.967, c2=-219.062 g=-308.408\n",
            ">305, c1=-241.471, c2=-222.045 g=-297.010\n",
            ">306, c1=-241.712, c2=-221.370 g=-252.528\n",
            ">307, c1=-243.401, c2=-224.949 g=-284.357\n",
            ">308, c1=-241.526, c2=-225.337 g=-283.597\n",
            ">309, c1=-244.478, c2=-225.790 g=-280.425\n",
            ">310, c1=-245.123, c2=-225.241 g=-286.224\n",
            ">311, c1=-245.716, c2=-226.545 g=-271.510\n",
            ">312, c1=-245.995, c2=-227.563 g=-255.753\n",
            ">313, c1=-247.236, c2=-226.972 g=-234.178\n",
            ">314, c1=-247.575, c2=-228.322 g=-301.162\n",
            ">315, c1=-248.004, c2=-225.539 g=-219.868\n",
            ">316, c1=-247.873, c2=-229.176 g=-204.191\n",
            ">317, c1=-250.892, c2=-232.975 g=-269.300\n",
            ">318, c1=-251.366, c2=-228.812 g=-267.151\n",
            ">319, c1=-251.370, c2=-227.590 g=-213.515\n",
            ">320, c1=-250.968, c2=-235.001 g=-245.857\n",
            ">321, c1=-252.818, c2=-233.850 g=-192.157\n",
            ">322, c1=-252.635, c2=-234.923 g=-201.484\n",
            ">323, c1=-255.068, c2=-237.460 g=-225.944\n",
            ">324, c1=-254.156, c2=-237.165 g=-239.373\n",
            ">325, c1=-257.037, c2=-233.863 g=-180.504\n",
            ">326, c1=-257.256, c2=-239.485 g=-226.122\n",
            ">327, c1=-258.721, c2=-240.022 g=-233.023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "y2YImk2Ro_Cf",
        "outputId": "8aa147b2-ef26-4463-bcd2-ffe5c076c830"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0d115415042e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'c1_hist' is not defined"
          ]
        }
      ]
    }
  ]
}